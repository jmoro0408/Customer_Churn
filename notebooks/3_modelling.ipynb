{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Display all columns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import f1_score, auc, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_dir = Path(Path.cwd().parent, \"data\", \"interim\")\n",
    "X_train = pd.read_pickle(Path(save_data_dir, \"X_train.pkl\"))\n",
    "y_train = pd.read_pickle(Path(save_data_dir, \"y_train.pkl\"))\n",
    "X_test = pd.read_pickle(Path(save_data_dir, \"X_test.pkl\"))\n",
    "y_test = pd.read_pickle(Path(save_data_dir, \"y_test.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 115) (80000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Thoughts\n",
    "## Potential models\n",
    "This is a binary classification task, and generally simpler models are best to start with. Therefore I will try logistic regression and a boosted decision tree initially. \n",
    "\n",
    "## Metrics\n",
    "This is an imbalanced dataset, so useful metrics will be F1 score and ROC/AUC, as well as precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel:\n",
    "    \"\"\"Class to hold info about different experiment metrics\"\"\"\n",
    "\n",
    "    def __init__(self, name, clf, X_train, y_train):\n",
    "        self.name = name\n",
    "        self.clf = clf\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def calc_f1_score(self):\n",
    "        self.f1 = np.mean(\n",
    "            cross_val_score(self.clf, self.X_train, self.y_train, cv=5, scoring=\"f1\")\n",
    "        )\n",
    "\n",
    "    def calc_precision(self):\n",
    "        self.precision = np.mean(\n",
    "            cross_val_score(\n",
    "                self.clf, self.X_train, self.y_train, cv=5, scoring=\"precision\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def calc_recall(self):\n",
    "        self.recall = np.mean(\n",
    "            cross_val_score(\n",
    "                self.clf, self.X_train, self.y_train, cv=5, scoring=\"recall\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def calc_auc(self):\n",
    "        self.auc = np.mean(\n",
    "            cross_val_score(\n",
    "                self.clf, self.X_train, self.y_train, cv=5, scoring=\"roc_auc\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_hyperparameters(self):\n",
    "        self.hyperparameters = self.clf.get_params()\n",
    "\n",
    "    def get_probabilities(self):\n",
    "        self.probabilities = (\n",
    "            self.clf.fit(self.X_train, self.y_train)\n",
    "            .predict_proba(X_train)\n",
    "            .argmax(axis=1)\n",
    "        )\n",
    "\n",
    "    def run_all(self):\n",
    "        self.calc_f1_score()\n",
    "        self.calc_precision()\n",
    "        self.calc_recall()\n",
    "        self.calc_auc()\n",
    "        self.get_hyperparameters()\n",
    "        self.get_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regeression\n",
    "## No tuning, default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "LogRegDefault = MLModel(\n",
    "    name=\"Logistic Regression default params\", clf=clf, X_train=X_train, y_train=y_train\n",
    ")\n",
    "LogRegDefault.run_all()\n",
    "models_dict[LogRegDefault.name] = LogRegDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19219009167682788"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegDefault.f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling. \n",
    "\n",
    "Since our F1 score is quite low I think hyperparameter tuning at this stage is pointless, as we would only expect a 5 - 10% increase in performance. \n",
    "At this point I'll try Under, over, and SMOTE sampling and check the results. \n",
    "\n",
    "## Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MoroJ\\OneDrive - AECOM\\Documents\\Python - General\\Customer_Churn\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "cc = ClusterCentroids(random_state=0)\n",
    "X_undersampled, y_undersampled = cc.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 79464), (1, 536)]\n",
      "[(0, 536), (1, 536)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))\n",
    "print(sorted(Counter(y_undersampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegDefaultUnderSampled = MLModel(\n",
    "    name=\"Logistic Regression default params - undersampled\",\n",
    "    clf=clf,\n",
    "    X_train=X_undersampled,\n",
    "    y_train=y_undersampled,\n",
    ")\n",
    "LogRegDefaultUnderSampled.run_all()\n",
    "models_dict[LogRegDefaultUnderSampled.name] = LogRegDefaultUnderSampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340085060100731"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegDefaultUnderSampled.f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 79464), (1, 536)]\n",
      "[(0, 79464), (1, 79464)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))\n",
    "print(sorted(Counter(y_oversampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegDefaultOverSampled = MLModel(\n",
    "    name=\"Logistic Regression default params - oversampled\",\n",
    "    clf=clf,\n",
    "    X_train=X_oversampled,\n",
    "    y_train=y_oversampled,\n",
    ")\n",
    "LogRegDefaultOverSampled.run_all()\n",
    "models_dict[LogRegDefaultOverSampled.name] = LogRegDefaultOverSampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8943985767606243"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegDefaultOverSampled.f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_SMOTE, y_SMOTE = smote_enn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 79464), (1, 536)]\n",
      "[(0, 79464), (1, 79464)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_train).items()))\n",
    "print(sorted(Counter(y_oversampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegDefaultSMOTE = MLModel(\n",
    "    name=\"Logistic Regression default params - oversampled\",\n",
    "    clf=clf,\n",
    "    X_train=X_SMOTE,\n",
    "    y_train=y_SMOTE,\n",
    ")\n",
    "LogRegDefaultSMOTE.run_all()\n",
    "models_dict[LogRegDefaultSMOTE.name] = LogRegDefaultSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8943985767606243"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegDefaultOverSampled.f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
